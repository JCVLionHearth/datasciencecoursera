Para aplicar un análisis de PCA (Análisis de Componentes Principales) y luego usar las componentes principales como entrada en un modelo de regresión lineal, sigamos estos pasos estructurados:

1. Conceptos clave

Antes de implementar el algoritmo, es importante entender los conceptos clave:

1.1. PCA (Análisis de Componentes Principales)

El PCA es una técnica de reducción de dimensionalidad que transforma un conjunto de variables correlacionadas en un nuevo conjunto de variables no correlacionadas llamadas componentes principales.
	•	Se basa en encontrar las direcciones de máxima varianza en los datos.
	•	Utiliza autovalores y autovectores de la matriz de covarianza para identificar estas direcciones.
	•	Las primeras componentes principales capturan la mayor parte de la variabilidad en los datos.

1.2. Regresión Lineal

Después de aplicar PCA, entrenaremos un modelo de regresión lineal, que tiene la forma:
￼
Donde:
	•	￼ es la variable objetivo.
	•	￼ son las variables explicativas (en este caso, los componentes principales).
	•	￼ son los coeficientes a estimar.
	•	￼ es el error residual.

2. Implementación paso a paso en Python

Vamos a seguir estos pasos:
	1.	Cargar los datos y preprocesarlos (normalización si es necesario).
	2.	Aplicar PCA para reducir la dimensionalidad de las variables explicativas.
	3.	Seleccionar el número óptimo de componentes.
	4.	Entrenar el modelo de regresión lineal usando las componentes principales como variables explicativas.
	5.	Evaluar el modelo.

Paso 1: Cargar y explorar los datos

Supongamos que tenemos un dataset con 8 variables explicativas (￼) y una variable objetivo ￼.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Cargar el dataset (ejemplo)
df = pd.read_csv("datos.csv")

# Separar variables explicativas (X) y objetivo (y)
X = df.iloc[:, :-1]  # Todas las columnas excepto la última
y = df.iloc[:, -1]   # Última columna (variable objetivo)

# Ver estructura de los datos
print(X.head())
print(y.head())

Paso 2: Normalización de los datos

El PCA es sensible a la escala de los datos, por lo que es importante estandarizar las variables.

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Normalizamos los datos

Paso 3: Aplicar PCA

Ahora aplicamos PCA y analizamos cuántas componentes principales son suficientes para explicar la varianza de los datos.

pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Explicación de varianza acumulada
explained_variance = np.cumsum(pca.explained_variance_ratio_)

# Gráfica de varianza acumulada
plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')
plt.xlabel('Número de Componentes Principales')
plt.ylabel('Varianza Explicada Acumulada')
plt.title('Selección del Número de Componentes')
plt.grid()
plt.show()

Si observamos que, por ejemplo, las primeras 4 componentes explican el 95% de la varianza, podemos reducir el número de variables a 4.

Paso 4: Transformar los datos con PCA

Aplicamos PCA con el número óptimo de componentes.

# Seleccionar el número óptimo de componentes (ejemplo: 4)
pca_opt = PCA(n_components=4)
X_pca_opt = pca_opt.fit_transform(X_scaled)

# Ver la nueva estructura de los datos
print(X_pca_opt.shape)  # Debería ser (n_muestras, 4)

Paso 5: Dividir los datos en entrenamiento y prueba

X_train, X_test, y_train, y_test = train_test_split(X_pca_opt, y, test_size=0.2, random_state=42)

Paso 6: Entrenar el modelo de regresión lineal

model = LinearRegression()
model.fit(X_train, y_train)

# Obtener coeficientes y sesgo (intercepto)
print("Coeficientes:", model.coef_)
print("Intercepto:", model.intercept_)

Paso 7: Evaluar el modelo

Ahora medimos el desempeño del modelo con R² y el error cuadrático medio (MSE).

# Predicciones
y_pred = model.predict(X_test)

# Evaluación del modelo
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("MSE:", mse)
print("R²:", r2)

3. Interpretación de resultados
	1.	Selección de componentes: Elegimos solo aquellas componentes que explican la mayor parte de la varianza sin perder demasiada información.
	2.	Coeficientes del modelo: Indican el impacto de cada componente en la variable objetivo.
	3.	R²: Indica qué porcentaje de la variabilidad de ￼ es explicada por el modelo.
	4.	MSE: Nos dice cuánto error promedio tiene la predicción.

4. Conclusiones y mejoras
	•	Si el R² es bajo, podemos probar aumentando el número de componentes o evaluando si PCA realmente mejora el modelo.
	•	Si el modelo no es preciso, podríamos probar otros métodos de reducción de dimensionalidad como LDA o Autoencoders.
	•	También podemos usar Regularización (Ridge o Lasso) para mejorar la regresión.

Este flujo te permitirá aplicar PCA correctamente antes de un modelo de regresión lineal. ¿Quieres que hagamos un análisis con tus propios datos?

La regularización en regresión lineal se usa para evitar el sobreajuste cuando hay colinealidad entre las variables o muchas dimensiones. Hay dos métodos principales:
	•	Ridge Regression (L2): Penaliza la suma de los coeficientes al cuadrado para reducir la varianza del modelo.
	•	Lasso Regression (L1): Penaliza la suma del valor absoluto de los coeficientes, lo que tiende a hacer que algunos sean exactamente 0 (selección de variables).

Dado que ya aplicaste el modelo de regresión lineal con y sin PCA, ahora agregaremos regularización a ambos casos.

1. Aplicar Regularización Ridge y Lasso

En Scikit-Learn, las implementaciones están en Ridge y Lasso dentro de sklearn.linear_model.

Sin PCA: Regresión Ridge y Lasso

from sklearn.linear_model import Ridge, Lasso

# Definir valores de alpha (parámetro de regularización)
alpha_value = 1.0  # Puedes ajustarlo

# Ridge Regression
ridge = Ridge(alpha=alpha_value)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)

# Lasso Regression
lasso = Lasso(alpha=alpha_value)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)

# Evaluación
print("Ridge R²:", r2_score(y_test, y_pred_ridge))
print("Lasso R²:", r2_score(y_test, y_pred_lasso))

	•	Un alpha bajo (~0.1) da un modelo más flexible.
	•	Un alpha alto (~10) fuerza más penalización y simplifica el modelo.

Con PCA: Regresión Ridge y Lasso

Si ya aplicaste PCA, puedes hacer lo mismo con las componentes principales:

# Ridge Regression con PCA
ridge_pca = Ridge(alpha=alpha_value)
ridge_pca.fit(X_train_pca, y_train)
y_pred_ridge_pca = ridge_pca.predict(X_test_pca)

# Lasso Regression con PCA
lasso_pca = Lasso(alpha=alpha_value)
lasso_pca.fit(X_train_pca, y_train)
y_pred_lasso_pca = lasso_pca.predict(X_test_pca)

# Evaluación
print("Ridge con PCA R²:", r2_score(y_test, y_pred_ridge_pca))
print("Lasso con PCA R²:", r2_score(y_test, y_pred_lasso_pca))

2. ¿Cómo elegir el mejor alpha?

Podemos usar validación cruzada con GridSearchCV para encontrar el mejor alpha:

from sklearn.model_selection import GridSearchCV

# Definir valores de alpha a probar
alphas = np.logspace(-3, 3, 10)  # De 0.001 a 1000

# GridSearch para Ridge
ridge_cv = GridSearchCV(Ridge(), param_grid={'alpha': alphas}, cv=5)
ridge_cv.fit(X_train, y_train)

# GridSearch para Lasso
lasso_cv = GridSearchCV(Lasso(), param_grid={'alpha': alphas}, cv=5)
lasso_cv.fit(X_train, y_train)

# Mejor alpha encontrado
print("Mejor alpha Ridge:", ridge_cv.best_params_)
print("Mejor alpha Lasso:", lasso_cv.best_params_)

Haz lo mismo con X_train_pca si usas PCA.

3. Interpretar los Coeficientes

Podemos analizar cómo Ridge y Lasso afectan los coeficientes:

print("Coeficientes Ridge:", ridge.coef_)
print("Coeficientes Lasso:", lasso.coef_)

	•	En Lasso, algunos coeficientes pueden volverse 0, lo que significa que el modelo eliminó esas variables.
	•	En Ridge, los coeficientes se reducen pero no se vuelven 0.

4. Comparación Final
	1.	Evalúa R² en cada modelo para ver cuál predice mejor.
	2.	Si Lasso elimina muchas variables, revisa si hace sentido reducir la dimensionalidad.
	3.	Si PCA con Ridge/Lasso mejora el modelo, úsalo para simplificar la cantidad de datos de entrada.

Resumen
	•	Regresión Ridge (L2): Reduce coeficientes pero no los hace 0. Útil si todas las variables aportan algo.
	•	Regresión Lasso (L1): Reduce coeficientes y puede hacer que algunos sean 0, eliminando variables irrelevantes.
	•	PCA + Regularización: Puede mejorar el rendimiento al reducir la multicolinealidad.
	•	Ajustar Alpha: Usa GridSearchCV para encontrar el mejor valor.

Con esto puedes comparar los modelos con y sin PCA, y decidir cuál es el mejor para tu problema. ¿Quieres hacer una comparación con tus datos y ver los resultados?
