Para aplicar un análisis de PCA (Análisis de Componentes Principales) y luego usar las componentes principales como entrada en un modelo de regresión lineal, sigamos estos pasos estructurados:

1. Conceptos clave

Antes de implementar el algoritmo, es importante entender los conceptos clave:

1.1. PCA (Análisis de Componentes Principales)

El PCA es una técnica de reducción de dimensionalidad que transforma un conjunto de variables correlacionadas en un nuevo conjunto de variables no correlacionadas llamadas componentes principales.
	•	Se basa en encontrar las direcciones de máxima varianza en los datos.
	•	Utiliza autovalores y autovectores de la matriz de covarianza para identificar estas direcciones.
	•	Las primeras componentes principales capturan la mayor parte de la variabilidad en los datos.

1.2. Regresión Lineal

Después de aplicar PCA, entrenaremos un modelo de regresión lineal, que tiene la forma:
￼
Donde:
	•	￼ es la variable objetivo.
	•	￼ son las variables explicativas (en este caso, los componentes principales).
	•	￼ son los coeficientes a estimar.
	•	￼ es el error residual.

2. Implementación paso a paso en Python

Vamos a seguir estos pasos:
	1.	Cargar los datos y preprocesarlos (normalización si es necesario).
	2.	Aplicar PCA para reducir la dimensionalidad de las variables explicativas.
	3.	Seleccionar el número óptimo de componentes.
	4.	Entrenar el modelo de regresión lineal usando las componentes principales como variables explicativas.
	5.	Evaluar el modelo.

Paso 1: Cargar y explorar los datos

Supongamos que tenemos un dataset con 8 variables explicativas (￼) y una variable objetivo ￼.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Cargar el dataset (ejemplo)
df = pd.read_csv("datos.csv")

# Separar variables explicativas (X) y objetivo (y)
X = df.iloc[:, :-1]  # Todas las columnas excepto la última
y = df.iloc[:, -1]   # Última columna (variable objetivo)

# Ver estructura de los datos
print(X.head())
print(y.head())

Paso 2: Normalización de los datos

El PCA es sensible a la escala de los datos, por lo que es importante estandarizar las variables.

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Normalizamos los datos

Paso 3: Aplicar PCA

Ahora aplicamos PCA y analizamos cuántas componentes principales son suficientes para explicar la varianza de los datos.

pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Explicación de varianza acumulada
explained_variance = np.cumsum(pca.explained_variance_ratio_)

# Gráfica de varianza acumulada
plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')
plt.xlabel('Número de Componentes Principales')
plt.ylabel('Varianza Explicada Acumulada')
plt.title('Selección del Número de Componentes')
plt.grid()
plt.show()

Si observamos que, por ejemplo, las primeras 4 componentes explican el 95% de la varianza, podemos reducir el número de variables a 4.

Paso 4: Transformar los datos con PCA

Aplicamos PCA con el número óptimo de componentes.

# Seleccionar el número óptimo de componentes (ejemplo: 4)
pca_opt = PCA(n_components=4)
X_pca_opt = pca_opt.fit_transform(X_scaled)

# Ver la nueva estructura de los datos
print(X_pca_opt.shape)  # Debería ser (n_muestras, 4)

Paso 5: Dividir los datos en entrenamiento y prueba

X_train, X_test, y_train, y_test = train_test_split(X_pca_opt, y, test_size=0.2, random_state=42)

Paso 6: Entrenar el modelo de regresión lineal

model = LinearRegression()
model.fit(X_train, y_train)

# Obtener coeficientes y sesgo (intercepto)
print("Coeficientes:", model.coef_)
print("Intercepto:", model.intercept_)

Paso 7: Evaluar el modelo

Ahora medimos el desempeño del modelo con R² y el error cuadrático medio (MSE).

# Predicciones
y_pred = model.predict(X_test)

# Evaluación del modelo
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("MSE:", mse)
print("R²:", r2)

3. Interpretación de resultados
	1.	Selección de componentes: Elegimos solo aquellas componentes que explican la mayor parte de la varianza sin perder demasiada información.
	2.	Coeficientes del modelo: Indican el impacto de cada componente en la variable objetivo.
	3.	R²: Indica qué porcentaje de la variabilidad de ￼ es explicada por el modelo.
	4.	MSE: Nos dice cuánto error promedio tiene la predicción.

4. Conclusiones y mejoras
	•	Si el R² es bajo, podemos probar aumentando el número de componentes o evaluando si PCA realmente mejora el modelo.
	•	Si el modelo no es preciso, podríamos probar otros métodos de reducción de dimensionalidad como LDA o Autoencoders.
	•	También podemos usar Regularización (Ridge o Lasso) para mejorar la regresión.

Este flujo te permitirá aplicar PCA correctamente antes de un modelo de regresión lineal. ¿Quieres que hagamos un análisis con tus propios datos?
