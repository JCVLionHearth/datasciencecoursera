import dash
from dash import dcc, html, dash_table
from dash.dependencies import Input, Output, State
import pandas as pd
import plotly.graph_objs as go
import base64
import io
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler

app = dash.Dash(__name__)

app.layout = html.Div([
    html.H1("Predicción de Series Temporales"),
    
    # Subir archivo de Excel
    dcc.Upload(
        id='upload-data',
        children=html.Button('Cargar archivo Excel'),
        multiple=False
    ),
    
    # Entrada para parámetros
    html.Div([
        dcc.Input(id='cutoff-date', type='text', placeholder='Fecha de Corte (YYYY-MM-DD)'),
        dcc.Input(id='num-series', type='number', placeholder='Número de Series'),
        dcc.Input(id='z-value', type='number', placeholder='Valor Z (Intervalo de Confianza)'),
        dcc.Input(id='num-months', type='number', placeholder='Meses a Predecir'),
        html.Button('Ejecutar Predicción', id='run-model')
    ]),
    
    # Salidas gráficas y tablas
    dcc.Graph(id='graph-prediction'),
    html.Div(id='table-summary')
])

# Función para procesar archivo Excel
def parse_contents(contents):
    content_type, content_string = contents.split(',')
    decoded = base64.b64decode(content_string)
    df = pd.read_excel(io.BytesIO(decoded))
    return df

# Función para entrenar y hacer predicciones con LSTM
def lstm_prediction(data, num_months):
    # Preprocesamiento
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))

    # Preparar los datos para LSTM
    X_train, y_train = [], []
    for i in range(60, len(scaled_data) - num_months):
        X_train.append(scaled_data[i-60:i, 0])
        y_train.append(scaled_data[i:i+num_months, 0])

    X_train, y_train = np.array(X_train), np.array(y_train)
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

    # Crear el modelo LSTM
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
    model.add(LSTM(units=50))
    model.add(Dense(num_months))

    # Compilar y entrenar el modelo
    model.compile(optimizer='adam', loss='mean_squared_error')
    model.fit(X_train, y_train, epochs=10, batch_size=32)

    # Hacer predicciones para el futuro
    inputs = scaled_data[len(scaled_data) - 60:].reshape(-1, 1)
    inputs = scaler.transform(inputs)
    X_test = []
    X_test.append(inputs)
    X_test = np.array(X_test)
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

    predicted_values = model.predict(X_test)
    predicted_values = scaler.inverse_transform(predicted_values).flatten()

    return predicted_values

# Callback para manejar la lógica y actualizar gráficos y tablas
@app.callback(
    [Output('graph-prediction', 'figure'),
     Output('table-summary', 'children')],
    [Input('run-model', 'n_clicks')],
    [State('upload-data', 'contents'),
     State('cutoff-date', 'value'),
     State('num-series', 'value'),
     State('z-value', 'value'),
     State('num-months', 'value')]
)
def update_output(n_clicks, uploaded_file, cutoff_date, num_series, z_value, num_months):
    if uploaded_file is None:
        return {}, ""

    # Procesar el archivo subido
    df = parse_contents(uploaded_file)

    # Filtrar datos hasta la fecha de corte (si se ha proporcionado)
    if cutoff_date:
        df = df[df['Fecha'] <= cutoff_date]

    # Extraer la columna de la serie temporal
    df_series = df['Valores']  # Ajusta el nombre según tu archivo Excel

    # Lógica de predicción (usamos LSTM como ejemplo)
    predicted_values = lstm_prediction(df_series, num_months)

    # Crear nuevo DataFrame con las predicciones
    pred_df = pd.DataFrame({
        'Fecha': pd.date_range(start=df['Fecha'].iloc[-1], periods=num_months+1, freq='M')[1:],
        'Predicción': predicted_values
    })

    # Crear gráfico interactivo
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df['Fecha'], y=df['Valores'], mode='lines', name='Histórico'))
    fig.add_trace(go.Scatter(x=pred_df['Fecha'], y=pred_df['Predicción'], mode='lines', name='Predicción'))

    # Crear tabla resumen
    summary_table = dash_table.DataTable(
        data=pred_df.to_dict('records'),
        columns=[{"name": i, "id": i} for i in pred_df.columns],
        style_table={'overflowX': 'scroll'}
    )
    
    return fig, summary_table

if __name__ == '__main__':
    app.run_server(debug=True)

¡Claro! A continuación te guiaré paso a paso para crear tu proyecto en **Visual Studio Code** utilizando **Python** y **Dash** para la aplicación de predicciones. Asegúrate de tener todo lo necesario instalado y configurado.

### 1. **Configurar tu entorno de desarrollo**

Primero, debes asegurarte de tener instaladas las siguientes herramientas:

- **Python** (Versión 3.x)
- **Visual Studio Code** (con la extensión de Python)
- **Pip** para instalar paquetes de Python

### 2. **Crear el entorno virtual**

En tu terminal de Visual Studio Code, crea un entorno virtual para el proyecto:

```bash
python -m venv venv
```

Activa el entorno virtual:

- En Windows:
  ```bash
  venv\Scripts\activate
  ```
- En macOS/Linux:
  ```bash
  source venv/bin/activate
  ```

### 3. **Instalar las dependencias**

Con el entorno activado, instala los paquetes necesarios usando `pip`:

```bash
pip install dash plotly pandas openpyxl
```

### 4. **Estructurar el proyecto**

Crea una estructura de carpetas y archivos dentro de tu proyecto:

```
/tu_proyecto_predicciones/
│
├── app.py           # Archivo principal para ejecutar la aplicación
├── assets/          # Carpeta para estilos CSS (si necesitas)
└── data/            # Carpeta para archivos Excel (si necesitas usar datos de prueba)
```

### 5. **Escribir el código de la aplicación (app.py)**

Copia el siguiente código dentro de `app.py`. Este código crea una aplicación básica en **Dash** que incluye la subida de archivos, inputs para los parámetros, y un gráfico interactivo usando **Plotly**:

```python
import dash
from dash import dcc, html, dash_table
from dash.dependencies import Input, Output, State
import pandas as pd
import plotly.graph_objs as go
import base64
import io

app = dash.Dash(__name__)

app.layout = html.Div([
    html.H1("Predicción de Series Temporales"),
    
    # Subir archivo de Excel
    dcc.Upload(
        id='upload-data',
        children=html.Button('Cargar archivo Excel'),
        multiple=False
    ),
    
    # Entrada para parámetros
    html.Div([
        dcc.Input(id='cutoff-date', type='text', placeholder='Fecha de Corte (YYYY-MM-DD)'),
        dcc.Input(id='num-series', type='number', placeholder='Número de Series'),
        dcc.Input(id='z-value', type='number', placeholder='Valor Z (Intervalo de Confianza)'),
        dcc.Input(id='num-months', type='number', placeholder='Meses a Predecir'),
        html.Button('Ejecutar Predicción', id='run-model')
    ]),
    
    # Salidas gráficas y tablas
    dcc.Graph(id='graph-prediction'),
    html.Div(id='table-summary')
])

# Función para procesar archivo Excel
def parse_contents(contents):
    content_type, content_string = contents.split(',')
    decoded = base64.b64decode(content_string)
    df = pd.read_excel(io.BytesIO(decoded), sheet_name="PQRS_Neiva_Orito")
    return df

# Callback para manejar la lógica y actualizar gráficos y tablas
@app.callback(
    [Output('graph-prediction', 'figure'),
     Output('table-summary', 'children')],
    [Input('run-model', 'n_clicks')],
    [State('upload-data', 'contents'),
     State('cutoff-date', 'value'),
     State('num-series', 'value'),
     State('z-value', 'value'),
     State('num-months', 'value')]
)
def update_output(n_clicks, uploaded_file, cutoff_date, num_series, z_value, num_months):
    if uploaded_file is None:
        return {}, ""
    
    df = parse_contents(uploaded_file)

    # Aquí va la lógica para ejecutar las predicciones (inserta tu modelo)
    # df contiene el DataFrame procesado del archivo Excel

    # Simulación de predicción con valores aleatorios
    pred = df.head(num_months)
    pred['Frecuencia_Predicha'] = pred.iloc[:, 1] + 10  # Ejemplo de predicción simple
    
    # Generar gráfico interactivo
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=pred['Fecha'], y=pred.iloc[:, 1], mode='lines', name='Histórico'))
    fig.add_trace(go.Scatter(x=pred['Fecha'], y=pred['Frecuencia_Predicha'], mode='lines', name='Predicción'))

    # Generar resumen en tabla
    table = dash_table.DataTable(
        data=pred.to_dict('records'),
        columns=[{"name": i, "id": i} for i in pred.columns],
        style_table={'overflowX': 'scroll'}
    )
    
    return fig, table

if __name__ == '__main__':
    app.run_server(debug=True)
```

### 6. **Ejecutar la aplicación**

Guarda todos los archivos y, en la terminal de **Visual Studio Code**, ejecuta:

```bash
python app.py
```

Si todo está configurado correctamente, verás un mensaje como este en tu terminal:

```
Dash is running on http://127.0.0.1:8050/
```

Abre tu navegador web y visita esa dirección (http://127.0.0.1:8050/), y verás la interfaz básica de tu aplicación con opciones para cargar un archivo Excel, ingresar los parámetros, y generar las predicciones.

### 7. **Mejoras adicionales**

#### a) **Agregar CSS para mejorar la apariencia**
Puedes personalizar la apariencia de la aplicación agregando un archivo CSS en la carpeta `assets/`. Por ejemplo, crea un archivo `assets/style.css` y agrega tus estilos.

#### b) **Añadir lógica de predicción con LSTM**
Sustituye la sección de "Simulación de predicción" en el código por tu modelo de LSTM o cualquier otro modelo de predicción que estés usando.

#### c) **Agregar la opción de descargar resultados en Excel**
Para permitir que los usuarios descarguen los resultados de las predicciones, puedes añadir un botón de descarga. Aquí tienes un ejemplo:

```python
import dash_core_components as dcc

# Agrega un botón de descarga en el layout
html.Button("Descargar Resultados", id="btn-download"),
dcc.Download(id="download-dataframe-xlsx")
```

Luego, en tu callback:

```python
@app.callback(
    Output("download-dataframe-xlsx", "data"),
    [Input("btn-download", "n_clicks")],
    [State('predictions', 'data')]
)
def download_excel(n_clicks, predictions):
    if n_clicks:
        return dcc.send_data_frame(predictions.to_excel, "predicciones.xlsx")
```

### 8. **Conclusión**

Con esto tendrás una aplicación web interactiva en Dash donde puedes cargar archivos Excel, realizar predicciones de series temporales, visualizar gráficos y tablas, y eventualmente descargar los resultados.

Hola, actualmente tengo el siguiente código para realizar el pronóstico de ciertos eventos a partir de redes neuronales, pero quiero convertirlo en una aplicación más dinámica e interactiva, quiero que el usuario cargue el archivo de excel, ingrese valores como:
la fecha de corte, número de series contenidas en la hoja de excel, el valor del z valor a partir del intervalo de confianza (actualmente es fijo a 1.96), el número de meses a predecir.
Quiero que la interfaz nme muestre las graficas con las predicciones y umbrales definidos (plotly).
Quiero que la interfaz también me muestre el resumen en una tabla con las predicciones, umbrales, promedios y alertas.
Necesito que la logica se mantentga pero mucho mas interactiva.
Quiero que en una interfaz se visualicen todas las predicciones de cada una de las series de cada hoja de mi excel.
Agrega más cosas que sean convenientes (descargar datos en excel)

Este es mi codigo actual

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
import random
np.random.seed(123)
tf.random.set_seed(123)
random.seed(123)
# Cargar datos desde el archivo Excel
file_path = 'C:/Users/E0305878/OneDrive - Ecopetrol S.A/Documentos/Proyectos Analitica/KRI redes Quejas pqrs/Serie_pqrs.xlsx'  # Ruta
df = pd.read_excel(file_path,sheet_name="PQRS_Neiva_Orito")

# Asegurarse de que las columnas están correctamente nombradas
df.columns = ['Fecha', 'Serie1', 'Serie2', 'Serie3', 'Serie4', 'Serie5','Serie6','Serie7','Serie8', 'Serie9', 'Serie10', 'Serie11', 'Serie12','Serie13','Serie14','Serie15','Serie16','Serie17']
# Convertir la columna 'Fecha' a un tipo datetime
df['Fecha'] = pd.to_datetime(df['Fecha'])

#Imputar valores faltantes
df=df.apply(lambda x: x.fillna(x.mean()),axis=0)

# Filtrar fecha de corte
corte_fecha = '2023-12-31'
df_futuro = df[df['Fecha'] > corte_fecha]
df = df[df['Fecha'] <= corte_fecha]


# Ordenar el DataFrame por fecha
df = df.sort_values('Fecha')
df_futuro = df_futuro.sort_values('Fecha')
# Función para crear secuencias de entrenamiento y etiquetas
def create_sequences(data, seq_length):
   xs, ys = [], []
   for i in range(len(data) - seq_length):
       x = data[i:i+seq_length]
       y = data[i+seq_length]
       xs.append(x)
       ys.append(y)
   return np.array(xs), np.array(ys)
# Parámetros
SEQ_LENGTH = 24  # Usar 24 meses para predecir el siguiente mes
EPOCHS = 50
NEURONAS=50
# Crear un archivo Excel para guardar las predicciones
writer = pd.ExcelWriter('predicciones_series_ejemplo.xlsx', engine='xlsxwriter')
# Bucle para cada serie
for serie in ['Serie1', 'Serie2', 'Serie3', 'Serie4', 'Serie5','Serie6','Serie7','Serie8', 'Serie9', 'Serie10', 'Serie11', 'Serie12','Serie13','Serie14','Serie15','Serie16','Serie17']:
   print(f"Procesando {serie}...")
   # Crear secuencias de entrenamiento y etiquetas
   X, y = create_sequences(df[serie].values, SEQ_LENGTH)
   # Dividir los datos en conjuntos de entrenamiento y prueba
   split = int(0.7 * len(X))
   X_train, X_test = X[:split], X[split:]
   y_train, y_test = y[:split], y[split:]
   # Redimensionar los datos para que sean compatibles con LSTM [samples, time steps, features]
   X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
   X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
   # Construir el modelo LSTM con Dropout
   model = Sequential()
   model.add(Dense(NEURONAS, activation='relu', input_shape=(SEQ_LENGTH,))) #capa con # neuronas para capturar la dependencia temporal
   model.add(Dropout(0.2)) #20% Esto ayuda a evitar el sobreajuste
   model.add(Dense(1)) #capa densa con una neurona para producir la predicción para el siguiente paso temporal
   model.compile(optimizer='adam', loss='mse')
   # Entrenar el modelo
   history = model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1)
   # Predecir en el conjunto de prueba
   y_pred = model.predict(X_test)
   # Calcular los errores y la desviación estándar
   errors = y_test - y_pred.flatten()
   std_error = np.std(errors)
   # Calcular el intervalo de confianza (IC 95%)
   confidence_interval_90 = 1.96 * std_error
   # Visualizar resultados del conjunto de prueba con intervalo de confianza
#    plt.figure(figsize=(10, 6))
#    plt.plot(range(len(y_test)), y_test, label='Actual')
#    plt.plot(range(len(y_pred)), y_pred, label='Predicted')
#    plt.fill_between(range(len(y_pred)),
#                     y_pred.flatten() - confidence_interval_90,
#                     y_pred.flatten() + confidence_interval_90,
#                     color='b', alpha=0.2, label='95% Confidence Interval')
#    plt.title(f"Predicción de la frecuencia de {serie} con IC 95% (ARNN)")
#    plt.xlabel("Tiempo")
#    plt.ylabel("Frecuencia")
#    plt.legend()
#    plt.savefig(f'prediccion_{serie}.png')
#    plt.show()
   # Predecir los próximos 12 meses con intervalos de confianza
   n_months = 12
   last_sequence = df[serie].values[-SEQ_LENGTH:]  # Tomar la última secuencia de longitud SEQ_LENGTH
   future_predictions = []
   future_confidences = []
   for _ in range(n_months):
       input_seq = last_sequence.reshape((1, SEQ_LENGTH, 1))
       next_pred = model.predict(input_seq)
       future_predictions.append(next_pred[0, 0])
       future_confidences.append(confidence_interval_90)
       last_sequence = np.append(last_sequence[1:], next_pred)  # Actualizar la secuencia para la siguiente predicción
   # Crear una lista de fechas para los próximos 12 meses
   last_date = df['Fecha'].iloc[-1]
   future_dates = [last_date + pd.DateOffset(months=i) for i in range(1, n_months + 1)]
   # Crear un DataFrame con las predicciones futuras
   future_df = pd.DataFrame({
       'Fecha': future_dates,
       f'Frecuencia_Predicha_{serie}': future_predictions,
       f'Limite_Superior_{serie}': np.array(future_predictions) + np.array(future_confidences),
       f'Limite_Inferior_{serie}': np.array(future_predictions) - np.array(future_confidences)
   })
   # Calcular el promedio histórico y la desviación estándar de los últimos 24 meses
   historical_window = 24
   ventana_total=df[serie].tail(historical_window)
   # Calcular los cuartiles y el rango intercuartil
   Q1 = ventana_total.quantile(0.25)
   Q3 = ventana_total.quantile(0.75)
   IQR = Q3 - Q1
   # Definir los límites para los valores atípicos
   lower_bound = Q1 - 1* IQR
   upper_bound = Q3 + 1* IQR
   # Filtrar los valores que no son atípicos
   filtered_data = ventana_total[(ventana_total >= lower_bound) & (ventana_total <= upper_bound)]
   # Calcular el promedio y la desviación estándar de los datos filtrados
   historical_mean = filtered_data.mean() #historical_mean = df[serie].tail(historical_window).mean()
   historical_std = filtered_data.std()   #historical_std = df[serie].tail(historical_window).std()
   # Añadir el promedio de los ultimos 24 meses al DataFrame
   future_df[f'promedioUlt24meses_{serie}'] = historical_mean
   #Umbral
   threshold0 = historical_mean - 0.524*historical_std #-40% de confianza
   threshold1 = historical_mean + 0.524*historical_std #+40% de confianza
   threshold2 = historical_mean + 1.645*historical_std #90% de confianza
   # Añadir el umbral al DataFrame
   future_df[f'UmbralMin_{serie}'] = threshold0
   future_df[f'UmbralMax_{serie}'] = threshold1
   # Visualizar las predicciones futuras con intervalo de confianza y umbral
   plt.figure(figsize=(10, 6))
   plt.plot(df['Fecha'], df[serie], label='Histórico')
   plt.plot(future_df['Fecha'], future_df[f'Frecuencia_Predicha_{serie}'], label='Predicción Futura', color='red')
   plt.plot(df_futuro['Fecha'], df_futuro[serie], label='Real',color='#008000',linestyle='dotted') #linestyle='-', '--', '-.', ':', 'None', ' ', '', 'solid', 'dashed', 'dashdot', 'dotted'
   plt.fill_between(future_df['Fecha'],
                    future_df[f'Limite_Inferior_{serie}'],
                    future_df[f'Limite_Superior_{serie}'],
                    color='r', alpha=0.2, label='95% Confidence Interval')
   plt.axhline(y=threshold0, color='orange', linestyle='--', label='Umbral de Alerta Min')
   plt.axhline(y=threshold1, color='red', linestyle='--', label='Umbral de Alerta Max')
   plt.axhline(y=historical_mean, color='green', linestyle='--', label='Promedio ult24m')
   # Marcar alertas
   for date, lstm_pred, lstm_conf in zip(future_df['Fecha'], future_predictions, future_confidences):
        if threshold0 < lstm_pred <= threshold1:
            plt.plot(date, lstm_pred, 'o',color='yellow')  # Punto amarillo para predicciones en zona de tolerancia
        elif lstm_pred > threshold1:
            plt.plot(date, lstm_pred, 'ro')  # Punto rojo para predicciones en zona extrema
        elif lstm_pred < threshold0:
            plt.plot(date, lstm_pred, 'go')  # Punto verde para predicciones por debajo del umbral minimo
   plt.title(f"Predicción futura de la frecuencia de {serie} con umbral (ARNN)")
   plt.xlabel("Fecha")
   plt.ylabel("Frecuencia")
   plt.legend()
   plt.savefig(f'prediccion_futura_{serie}.png')
   plt.show()
   # Guardar las predicciones futuras en el archivo Excel
   future_df.to_excel(writer, sheet_name=f'Predicciones_{serie}', index=False)
# Guardar y cerrar el archivo Excel
writer.close()
print("Archivo Excel generado correctamente.")

